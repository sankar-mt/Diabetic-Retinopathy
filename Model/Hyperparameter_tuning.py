# -*- coding: utf-8 -*-
"""HPTuned_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BJg3eaBo1gtGmGn4p_aPuqtBUw1raEyN
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.applications.resnet50 import ResNet50
import numpy as np
import pandas as pd
# %load_ext tensorboard
!rm -rf ./logs/

import tensorflow as tf
from tensorboard.plugins.hparams import api as hp

!rm -rf ./logs/

import tensorflow as tf
from tensorboard.plugins.hparams import api as hp

HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.4,0.5]))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd' ,'rmsprop']))
HP_LEARNING_RATE= hp.HParam('learning_rate', hp.Discrete([0.001, 0.0005]))
METRIC_ACCURACY = 'accuracy'

with tf.summary.create_file_writer('/content/drive/MyDrive/logs/hparam_tuning').as_default():
  hp.hparams_config(
    hparams=[HP_DROPOUT, HP_OPTIMIZER, HP_LEARNING_RATE],
    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]
  )

import pathlib

train_dir = pathlib.Path('/content/drive/MyDrive/MAIN/train')
val_dir = pathlib.Path('/content/drive/MyDrive/MAIN/val')
image_count = len(list(train_dir.glob('*/*.png')))
print(image_count)
image_count = len(list(val_dir.glob('*/*.png')))
print(image_count)

batch_size = 50
img_height = 256
img_width = 256
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  train_dir,
  validation_split=None,
  seed=123,shuffle=True,color_mode="grayscale",
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  val_dir,
  validation_split=None,
  seed=123,shuffle=True,color_mode="grayscale",
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

from keras.models import Model
from keras.layers import Dense, Dropout, Flatten
num_classes = 5
def train_test_model(hparams):
    data_augmentation = keras.Sequential(
        [
            tf.keras.layers.experimental.preprocessing.Rescaling(.1/255),
            tf.keras.layers.experimental.preprocessing.RandomRotation(0.3),
            tf.keras.layers.experimental.preprocessing.RandomZoom(0.3),
        
        ]
    )

    base_model = ResNet50(weights = None, include_top=False, input_shape=(256, 256, 1))
    x = base_model.output
    x = data_augmentation(x)
    x = Flatten()(x)
    x = Dropout(hparams[HP_DROPOUT])(x)
    x = Dense(32, activation='relu')(x)    
    x = Dense(16, activation='relu')(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    optimizer = hparams[HP_OPTIMIZER]
    learning_rate = hparams[HP_LEARNING_RATE]
    if optimizer == "adam":
        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer == "sgd":
        optimizer = tf.optimizers.SGD(learning_rate=learning_rate)
    elif optimizer=='rmsprop':
        optimizer = tf.optimizers.RMSprop(learning_rate=learning_rate)
    else:
        raise ValueError("unexpected optimizer name: %r" % (optimizer_name,))

    # This is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)
    model.compile(
      optimizer=optimizer,
      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=['accuracy'])
    history = model.fit(
                        train_ds,
                        validation_data=val_ds,
                        epochs=20
                      )
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    print(history.history['accuracy'][19])
    accuracy = history.history['accuracy'][19]
    return accuracy

def run(run_dir, hparams):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams)  # record the values used in this trial
    accuracy = train_test_model(hparams)
    print(accuracy)
    tf.summary.scalar(METRIC_ACCURACY,accuracy, step=1)

session_num = 0
for dropout_rate in HP_DROPOUT.domain.values:
    for optimizer in HP_OPTIMIZER.domain.values:
         for learning_rate in HP_LEARNING_RATE.domain.values:
          hparams = {
              HP_DROPOUT: dropout_rate,
              HP_OPTIMIZER: optimizer,
              HP_LEARNING_RATE: learning_rate
          }
          run_name = "run-%d" % session_num
          print('--- Starting trial: %s' % run_name)
          print({h.name: hparams[h] for h in hparams})
          run('/content/drive/MyDrive/logs/hparam_tuning/' + run_name, hparams)
          session_num += 1

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/logs/hparam_tuning